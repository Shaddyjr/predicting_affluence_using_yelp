{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Areas of Affluence using Yelp Pricing Data\n",
    "\n",
    "#### Authors: \n",
    "- Eddie Yip [LinkedIn](https://www.linkedin.com/in/eddie-yip-2a37324b/) | [Medium](https://medium.com/@eddie.yip2)\n",
    "- Hadi Morrow [LinkedIn](https://www.linkedin.com/in/hadi-morrow-4b94164b/) | [GitHub](https://github.com/HadiMorrow) | [Medium](https://medium.com/@hadi.a.morrow)\n",
    "- Mahdi Shadkam-Farrokhi: [GitHub](https://github.com/Shaddyjr) | [Medium](https://medium.com/@mahdis.pw) | [http://mahdis.pw](http://mahdis.pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement [Hadi]\n",
    "\n",
    "While affluence should never be a factor when choosing to provide disaster aid or not, we must consider the following:\n",
    "\n",
    "- On the assumption that affluence plays a role, one might relate affluency to preparedness. Those who can afford to will always look out for their families at any cost. Those who can not might not be able to prepare as well due to the fact that it is not an option. \n",
    "\n",
    "- On the assumption that affluence is not part of a majority class, if we should be miopic with our search efforts we might want to consider saving the masses, those living in tight coridors and those with little to no income. If effect those most suseptible to losing their lives in a major disaster. \n",
    "\n",
    "- Using tax data we aim to show that using YELP data dollar signs is enough to predict where we might want to quickly and accuratly align our efforts. \n",
    "\n",
    "New Light Technologies as our audience, we hope to show that while using expensive and hard to handle data such as tax data can be more precise, a quick and dirty aproach could be to simply sord though the dollar signs data on yelp. \n",
    "\n",
    "---\n",
    "[Hadi] - Excellent write up! Here are some suggestions.\n",
    "1. It would be nice for the reader if we define 'affluence' here at the start. What do we consider \"affluent\" in our data (I think we mentioned 15% of the area code?)?\n",
    "2. As a reader, it would be VERY compelling to have an actual case where a natural disaster occured and the affluent areas weren't affected. If possible, research 1 or 2 cases when affluent areas were better prepared for natural disasters - this will help prove our predictive model has a real use case.\n",
    "3. Tying into the use case, it might be helpful to mention a realistic disaster scenario when only having Yelp! price data would be useful. Like, there's an emergency and there's little time to pull granular information about the area, but knowing the yelp reviews for an area, allows first-responders to know which areas their efforts will have the most impact\n",
    "4. We need to mention which metric we're going to use and why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary [Mahdi]\n",
    "\n",
    "- Difficulty gathering data\n",
    "- Prompt confusing regarding \"affluence\"\n",
    "- Other projects used outside data as metric\n",
    "- We pulled from API and didn't use old data, which was challenging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Gathering Data](#Gathering-Data)\n",
    "- [Loading Data](#Loading-Data)\n",
    "- [Preliminary Exploratory Data Analysis](#Preliminary-Exploratory-Data-Analysis)\n",
    "- [Cleaning the Data](#Cleaning-the-Data)\n",
    "- [Feature Engineering](#Feature-Engineering)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "- [Model Preparation](#Model-Preparation)\n",
    "- [Model Selection](#Model-Selection)\n",
    "- [Model Evaluation](#Model-Evaluation)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)\n",
    "- [Source Documentation](#Source-Documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "We got Yelp data using the API - link \n",
    " \n",
    "We got IRS data using - source [Eddie]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "- [All]\n",
    "- For map visual, will need [Basemap](https://rabernat.github.io/research_computing/intro-to-basemap.html)\n",
    "[source 2](https://jakevdp.github.io/PythonDataScienceHandbook/04.13-geographic-data-with-basemap.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import columnExpander\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "# from mpl_toolkits.basemap import Basemap # this model has messy import process\n",
    "\n",
    "random_state = 6988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = \"./data/total_merge_2.csv\"\n",
    "df_yelp = pd.read_csv(data_file_path, index_col = 0)\n",
    "df_yelp.reset_index(drop=True, inplace = True) # same indeces were merged using multiple API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Exploratory Data Analysis\n",
    "- [All]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_null = df_yelp.isnull().sum()\n",
    "sum_null[sum_null > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have many missing values in the data, however many of the columns are not meaningful for our problem and these columns can be safely dropped.\n",
    "\n",
    "Also, `categories`, `location`, and `transactions` are compressed data columns and will need to be unpacked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "- [Mahdi] one person for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[\"price\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to drop null prices from analysis as this is the key indicator we're looking to predict with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.dropna(subset=[\"price\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Yelp Price to ordinal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['price'] = df_yelp['price'].map({'$': 1, '$$': 2, '$$$': 3, '$$$$':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['price'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unneccessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keepers = ['categories','id', 'location', 'price', 'rating', 'review_count', 'transactions', 'coordinates']\n",
    "df_yelp = df_yelp[keepers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_sting_dict(string, keys):\n",
    "    if len(string) == 0:\n",
    "        return None\n",
    "    dic = literal_eval(string)\n",
    "    out = {}\n",
    "    for key in keys:\n",
    "        out[key] = dic.get(key)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"location\"\n",
    "keys = [\"zip_code\", \"city\", 'state']\n",
    "zips_and_cities = df_yelp[location].map(lambda string: get_keys_from_sting_dict(string, keys))\n",
    "\n",
    "for key in keys:\n",
    "    df_yelp[key] = [pair[key] for pair in zips_and_cities]\n",
    "    \n",
    "df_yelp.drop(columns=[location], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for NYC-only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed non-NY state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = df_yelp[df_yelp['state'] == \"NY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing zip codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[df_yelp['zip_code'] == \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these locations were found using Google Maps; their zip codes were are input manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.loc[df_yelp[\"id\"] == \"ECY0sIYxPJio81dteqiMhg\",\"zip_code\"] = \"10007\"\n",
    "df_yelp.loc[df_yelp[\"id\"] == \"6u5cnsN35mJz24HMQ9pfFw\",\"zip_code\"] = \"10314\"\n",
    "df_yelp.loc[df_yelp[\"id\"] == \"BilbRcNQXKmcBFvLm4gxAQ\",\"zip_code\"] = \"11372\"\n",
    "df_yelp.loc[df_yelp[\"id\"] == \"jZzbV6SRt9FXdCoziNv5xw\",\"zip_code\"] = \"11373\"\n",
    "df_yelp.loc[df_yelp[\"id\"] == \"9XXQ2w3DCFytsjjb3KmU7g\",\"zip_code\"] = \"10017\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove by NYC zip\n",
    "We used the range of zip codes designated for NYC - [source](https://www.nycbynatives.com/nyc_info/new_york_city_zip_codes.php)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_zip = 10001\n",
    "max_zip = 11697\n",
    "\n",
    "df_yelp['zip_code'] = df_yelp['zip_code'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = df_yelp[(df_yelp['zip_code'] >= min_zip) & (df_yelp['zip_code'] <= max_zip)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing low frequency businesses\n",
    "\n",
    "In order to ensure a stable model, we're only going to consider businesses in our data with a moderate frequency of 15. This number is arbitrary, but considered a decent representation in data science. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_treshold = 15\n",
    "zip_counts = df_yelp[\"zip_code\"].value_counts()\n",
    "low_count_zip_counts = zip_counts[zip_counts < freq_treshold].index\n",
    "high_count_zip_counts = zip_counts[zip_counts >= freq_treshold].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = df_yelp[[zip_code in high_count_zip_counts for zip_code in df_yelp[\"zip_code\"]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = \"coordinates\"\n",
    "coord_keys = [\"latitude\", \"longitude\"]\n",
    "lat_and_long = df_yelp[coordinates].map(lambda string: get_keys_from_sting_dict(string, coord_keys))\n",
    "\n",
    "for key in coord_keys:\n",
    "    df_yelp[key] = [pair[key] for pair in lat_and_long]\n",
    "\n",
    "df_yelp.drop(columns=[coordinates], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[coord_keys].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the latitude and longitude, we see some points that do not appear to be in the New York City area, around 40.7 and -73.9, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrong latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[(df_yelp[\"latitude\"] > 41) | (df_yelp[\"latitude\"] < 40)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data point is from Scarsdale NY, which is not within the city limits. This data point will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = df_yelp[(df_yelp[\"latitude\"] <= 41) & (df_yelp[\"latitude\"] >= 40)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrong longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[(df_yelp[\"longitude\"] > -72) | (df_yelp[\"longitude\"] < -75)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking up these businesses, it is clear they were given a positive longitude when they are actually supposed to be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.loc[(df_yelp[\"longitude\"] > -72) | (df_yelp[\"longitude\"] < -75),\"longitude\"] = np.negative(df_yelp[(df_yelp[\"longitude\"] > -72) | (df_yelp[\"longitude\"] < -75)][\"longitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing categories\n",
    "\n",
    "The `categories` column is condensed as a dictionary. By selecting the `alias` and reassigning the column as a string of categories, this process prepares the column for later conversion into dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_dict_to_string(string, key):\n",
    "    return \",\".join([dic[key] for dic in literal_eval(string)])\n",
    "\n",
    "df_yelp[\"categories\"] = df_yelp[\"categories\"].map(lambda s: convert_string_dict_to_string(s,\"alias\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing transactions\n",
    "\n",
    "Similarly, the `transactions` column is condensed as a list, which will also be parsed and prepped for later conversion into dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_list_to_string(string):\n",
    "    return \",\".join(literal_eval(string))\n",
    "\n",
    "df_yelp[\"transactions\"] = df_yelp[\"transactions\"].map(convert_string_list_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values - this is a complete dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning IRS Dataset [Hadi]\n",
    "These data were collected directly from the IRS website ([source](https://www.irs.gov/statistics/soi-tax-stats-individual-income-tax-statistics-2016-zip-code-data-soi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irs = pd.read_csv('./data/irs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irs[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_zips = list(set(df_yelp['zip_code']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str_num(str_num):\n",
    "    '''Returns integer of input string with commas removed'''\n",
    "    return int(str_num.replace(',',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affluency_rates = []\n",
    "found_zips = []\n",
    "missing_zips = []\n",
    "returns_col_name = 'Number of returns'\n",
    "\n",
    "for zip_code in yelp_zips:\n",
    "    try:\n",
    "        sub_df               = df_irs[df_irs.iloc[:,0] == str(zip_code)]\n",
    "        \n",
    "        affluent_irs_returns = clean_str_num(sub_df[returns_col_name].iloc[-1])\n",
    "        total_irs_returns    = clean_str_num(sub_df[returns_col_name].iloc[0])\n",
    "        affluent_rate        = affluent_irs_returns / total_irs_returns\n",
    "        \n",
    "        affluency_rates.append(affluent_rate)\n",
    "        found_zips.append(zip_code)\n",
    "    except Exception as e:\n",
    "        missing_zips.append(zip_code)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_zips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 26 zip codes in the yelp data that were not found in the IRS dataset.  \n",
    "These associated datapoint will be dropped, as they have not target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affluency_df = pd.DataFrame(data = {\"zip_code\": found_zips, \"affluency_rate\":affluency_rates})\n",
    "affluency_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Yelp and IRS dataset\n",
    "Merging the yelp dataset with the IRS dataset will drop those observations with missing zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_yelp, affluency_df, on = \"zip_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_int = [\"review_count\",\"rating\"]\n",
    "df[convert_to_int] = df[convert_to_int].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left with 9809 complete data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "- [All]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction term: price\\*rating\n",
    "This interaction term represents a scoring value that allows for relative comparisons between businesses with varying levels of price and rating.\n",
    "\n",
    "For example: a restaurant with 4 stars and \"\\\\$\" price is not as impressive as a restaurant with 4 stars and \"\\\\$\\\\$\\\\$\\\\$\" price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"price*rating\"] = df[\"price\"] * df[\"rating\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `ListColumnExpander` class\n",
    "\n",
    "- [Mahdi] Explain a bit about how this works and why you made it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansion_columns = [\"categories\",\"transactions\"]\n",
    "lce = columnExpander.ListColumnExpander(expansion_columns)\n",
    "\n",
    "dummy_df = pd.DataFrame(lce.fit_transform(df).toarray(), columns=lce.get_feature_names())\n",
    "\n",
    "complete_df = pd.concat([df.drop(columns = expansion_columns), dummy_df], axis=1)\n",
    "complete_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Affluency Threshold\n",
    "Affluency = when a zip code has 15% of its population file and IRS return of $$200k or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affluency_thresh = .15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df[\"is_affluent\"] = (complete_df[\"affluency_rate\"] >= affluency_thresh).astype(int)\n",
    "complete_df[\"is_affluent\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 30% of all reported IRS returns in New York City count as being affluent, according to our definition.\n",
    "\n",
    "This leads our data to be somewhat unbalanced, which we need to keep in mind when evaluating our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Latitude and Longitude into clusters\n",
    "We decided to reduce the latitude and longitude variables into a single column using $K$-means clustering as a way of representing more local neighborhoods.\n",
    "\n",
    "An optimum $k$ will be determined using the model's silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmean_sil_score(data, k, random_state):\n",
    "    return silhouette_score(data,KMeans(n_clusters = k, n_jobs = -1, random_state = random_state).fit(data).labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up K-means scree plot\n",
    "max_k = 14\n",
    "\n",
    "lat_long_df = StandardScaler().fit_transform(complete_df[[\"latitude\",\"longitude\"]]) # not 100% necessary to scale\n",
    "scores = []\n",
    "best_k = {\"k\":0,\"score\":0}\n",
    "all_k  = range(2, max_k + 1)\n",
    "best_km = None\n",
    "for k in all_k:\n",
    "    score = get_kmean_sil_score(lat_long_df, k, random_state)\n",
    "    scores.append(score)\n",
    "    if score > best_k[\"score\"]:\n",
    "        best_k[\"score\"] = score\n",
    "        best_k[\"k\"] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Scree plot of lat/long $k$-means\")\n",
    "sns.scatterplot(all_k, scores)\n",
    "plt.plot(best_k[\"k\"],best_k[\"score\"], \"ro\")\n",
    "plt.text(best_k[\"k\"] + .5 ,best_k[\"score\"], \"k = {}\\nscore = {:.2%}\".format(best_k['k'],best_k['score']));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the silhouette score, an optimum number of clusters for longitude and latitude occur is 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters= best_k[\"k\"], random_state = random_state)\n",
    "km.fit(lat_long_df)\n",
    "complete_df[\"location_cluster\"] = km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating \"Incomplete\" Dataset\n",
    "As a proof of concept and practical use-case, we're interested in allowing a user to input a NYC zip code and quickly know the affluency of the area. This would allow emergency responders to quickly assess which areas are in the most need. This application is explained in more detail in the [Data Query](#Data-Query) section.\n",
    "\n",
    "Having an incomplete dataset (which includes observations without affluency) allows us to pass the pertinent information about a zip code through our model to make a prediction for the affluency of the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_incomplete_df = pd.merge(df_yelp, affluency_df, on = \"zip_code\", how = 'outer')\n",
    "temp_incomplete_df[convert_to_int] = temp_incomplete_df[convert_to_int].astype(int)\n",
    "temp_incomplete_df[\"price*rating\"] = temp_incomplete_df[\"price\"] * temp_incomplete_df[\"rating\"]\n",
    "\n",
    "dummy_incomplete_df = pd.DataFrame(lce.transform(temp_incomplete_df).toarray(), columns=lce.get_feature_names())\n",
    "\n",
    "incomplete_df = pd.concat([temp_incomplete_df.drop(columns = expansion_columns), dummy_incomplete_df], axis=1)\n",
    "\n",
    "incomplete_df[\"location_cluster\"] = km.predict(incomplete_df[[\"latitude\",\"longitude\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "- [Mahdi] and [Hadi] killer graphs and visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET THRESHOLD FOR NUMBER OF OBSERVATIONS TO SHRINK IMAGE\n",
    "order_of_cities = complete_df.groupby(\"city\")[\"affluency_rate\"].mean().sort_values().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,28))\n",
    "sns.boxplot(data = complete_df, y = \"city\", x = \"affluency_rate\", orient=\"h\", order=order_of_cities);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see some correlation between `city` and the affluency rate of the city. \n",
    "\n",
    "However, using subject knowledge we know _these city designations are inconsistent and ambiguous_. For example, \"New York\" and \"New York City\" and \"Manhattan\" and \"new york\" and \"New york\" could be considered synonymous.\n",
    "\n",
    "__Therefore we will not be includeing `city` in our modeling process.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========\n",
    "# MESSY BELOW HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing location clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_of_clusters = complete_df.groupby(\"location_cluster\")[\"affluency_rate\"].mean().sort_values().index\n",
    "\n",
    "colors = ['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe']\n",
    "\n",
    "neighborhoods = [\"Bronx\", \"Upper Bkln\", \"Astoria\", \"Staten Island\", \"Upper Mhtn\", \"Lower Bkln\", \"Lower Mhtn\", \"Jamaica\", \"Flushing\", \"New Rochelle\\n&\\nYonkers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,8))\n",
    "g = sns.boxplot(\n",
    "    data = complete_df, \n",
    "    x = \"location_cluster\", \n",
    "    y = \"affluency_rate\", \n",
    "    order = order_of_clusters,\n",
    "    palette = colors,\n",
    ");\n",
    "\n",
    "g.set(xticklabels = np.array(neighborhoods)[order_of_clusters]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing affluency by location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "sns.scatterplot(\n",
    "    data = complete_df, \n",
    "    y = \"latitude\", \n",
    "    x = \"longitude\", \n",
    "    hue = \"location_cluster\",\n",
    "    hue_order = order_of_clusters,\n",
    "    palette = colors,\n",
    "    s = 16, \n",
    "    legend = False\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://matplotlib.org/basemap/api/basemap_api.html\n",
    "# https://matplotlib.org/basemap/users/geography.html\n",
    "# https://basemaptutorial.readthedocs.io/en/latest/backgrounds.html#arcgisimage\n",
    "plt.figure(figsize=(lon_diff * scale, lat_diff * scale))\n",
    "m = Basemap(\n",
    "    resolution = None, \n",
    "    llcrnrlat = complete_df[\"latitude\"].min() - lat_pad,\n",
    "    llcrnrlon = complete_df[\"longitude\"].min() - lon_pad,\n",
    "    urcrnrlat = complete_df[\"latitude\"].max() + lat_pad,\n",
    "    urcrnrlon = complete_df[\"longitude\"].max() + lat_pad,\n",
    "    epsg = 3395\n",
    ")\n",
    "\n",
    "\n",
    "m.arcgisimage(service =\"World_Shaded_Relief\", xpixels = 2000)\n",
    "m.scatter(lon, lat, latlon=True, \n",
    "    s = 1,\n",
    "    c = np.array(colors)[complete_df[\"location_cluster\"]]\n",
    ");\n",
    "\n",
    "\n",
    "# ADD COLOR BARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping geograph\n",
    "lat    = complete_df['latitude'].values\n",
    "lon    = complete_df['longitude'].values\n",
    "price  = complete_df['price'].values\n",
    "rating = complete_df['rating'].values\n",
    "\n",
    "padding = .05\n",
    "lat_diff = complete_df[\"latitude\"].max() - complete_df[\"latitude\"].min()\n",
    "lat_pad = (lat_diff) * padding\n",
    "\n",
    "lon_diff = complete_df[\"longitude\"].max() - complete_df[\"longitude\"].min()\n",
    "lon_pad = (lon_diff) * padding\n",
    "\n",
    "scale = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# https://matplotlib.org/basemap/api/basemap_api.html\n",
    "# https://matplotlib.org/basemap/users/geography.html\n",
    "# https://basemaptutorial.readthedocs.io/en/latest/backgrounds.html#arcgisimage\n",
    "plt.figure(figsize=(lon_diff * scale, lat_diff * scale))\n",
    "m = Basemap(\n",
    "    resolution = None, \n",
    "    llcrnrlat = complete_df[\"latitude\"].min() - lat_pad,\n",
    "    llcrnrlon = complete_df[\"longitude\"].min() - lon_pad,\n",
    "    urcrnrlat = complete_df[\"latitude\"].max() + lat_pad,\n",
    "    urcrnrlon = complete_df[\"longitude\"].max() + lat_pad,\n",
    "    epsg = 3395\n",
    ")\n",
    "\n",
    "\n",
    "m.arcgisimage(service =\"World_Shaded_Relief\", xpixels = 2000)\n",
    "m.scatter(lon, lat, latlon=True, \n",
    "    s = 1,\n",
    "    cmap ='cool', \n",
    "    c = complete_df[\"affluency_rate\"]\n",
    ");\n",
    "\n",
    "# Add colorbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MESSY ABOVE HERE\n",
    "# ==========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/Loading Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving cleaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# complete_df.to_csv(\"./data/clean_data.csv\")\n",
    "# incomplete_df.to_csv(\"./data/clean_incomplete_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading cleaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import columnExpander\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.read_csv(\"./data/clean_data.csv\", index_col = 0)\n",
    "incomplete_df = pd.read_csv(\"./data/clean_incomplete_data.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling: Two Paths\n",
    "With almost 300 features, we were interested in finding an optimal model to determine how significant Yelp prices are in predicting affluency in an area******* (This is different from affluency of individual business)\n",
    "\n",
    "We considered feature elimination as well as extraction, and decided on splitting the difference. We tried fitting models without altering the data, [PATH 1](#PATH-1:-Default-dataset), as well as trying to fit models with a reduced dataset, [PATH 2](#PATH-2:-Feature-Reduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df[\"is_affluent\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline accuracy for the data is about 71.7% being not affluent.\n",
    "\n",
    "**** Being mindful of our metric - accuracy not what we're using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining scorer object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have GridSearchCV properly optimize for our metric, we have to create a sklearn scorer object and provide it as the `scorer` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec(y_true,y_pred):\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true,y_pred).ravel()\n",
    "    specificity = (TN)/(TN+FP)\n",
    "    return specificity\n",
    "\n",
    "specificity_scorer = make_scorer(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Receiving Operating Characteristics Curve Function\n",
    "\n",
    "We want to define a the function to draw the ROC curve for each model for a visual evaluation for their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATH 1: Default dataset\n",
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns = [\n",
    "    'id',\n",
    "    'zip_code',\n",
    "    'city',\n",
    "    'state',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'affluency_rate',\n",
    "    'transactions_',\n",
    "    'location_cluster'\n",
    "]\n",
    "target = 'is_affluent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = complete_df.drop(columns=remove_columns+[target])\n",
    "y = complete_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom `gridSearchHelper` function\n",
    "This function simply includes some \"quality of life\" functionality to help streamline the modeling process, such as printing the training score, test, score and and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearchHelper(estimator, params, X, y, standardize = False):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = random_state, stratify = y)\n",
    "\n",
    "    if standardize:\n",
    "        sc      = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test  = sc.transform(X_test)\n",
    "\n",
    "    gs = GridSearchCV(estimator, params, cv = 5, n_jobs=-1, scoring = specificity_scorer, verbose=2)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Train score:\", gs.best_score_)\n",
    "    print(\"Test score:\", gs.score(X_test, y_test))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, gs.predict(X_test)))\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {'C': [1.023292992280754],\n",
    "          'penalty': ['l2']}\n",
    "\n",
    "gridSearchHelper(LogisticRegression(), lr_params, X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial model to predict affluency is the Logistic Model. We want to use the simpliest model as a benchmark to see how it performs. Our model incorporates most of the features engineered with the exception of redundant features removed during the model preparation process. Utilizing gridsearch, we discovered the optimal parameters that gave the highest score for specificity. We want to use specificity as the primary metric because it would not be benefit for improverished neighborhoods to be labeled as affluent in the event of disaster. It can cause the incorrect distribution of resource and aid for those particular neighborhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {'n_neighbors': [11],\n",
    "              'algorithm': ['ball_tree']}\n",
    "\n",
    "gridSearchHelper(\n",
    "    KNeighborsClassifier(),\n",
    "    knn_params, \n",
    "    X, y, \n",
    "    standardize = True # Must standardize for KNN\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next model will be K-Nearest Neighbors where it functions by grouping up data depending on their proximity. The specificity score here did not surpass the Logistic Model but it was not far off. However, because it did not exceed expections, we will not utilize these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_params = {'max_depth': [3],\n",
    "             'min_samples_split': [2],\n",
    "             'min_samples_leaf': [1],\n",
    "             'max_features': [ None]}\n",
    "\n",
    "gs_dt = gridSearchHelper(\n",
    "    DecisionTreeClassifier(random_state = random_state), \n",
    "    dt_params, \n",
    "    X, y\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DecisionTree Model has delivered desireable results as our confusion has showed that we have greatly reduced our false positives and false negatives. In addition to the low amount of false positives, the model also presented low amount of variance which shows that the data provided had many meaningful and important features that provided this analysis with a fair amount of accuracy. As we continue to explore the models below, we will evaluate other tree-type models and see if improvements can be made whether by either tuning parameters or selecting more robust models. However, with the DecisionTree achieving a score that we see above, we might not see much improvements, if any, that can better examine the data and provide additional accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagged Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_params = {'n_estimators': [10, 20, 30],\n",
    "             'max_features': [10, 20, 30, 40, 50]}\n",
    "\n",
    "model_bg = gridSearchHelper(\n",
    "    BaggingClassifier(random_state = random_state), \n",
    "    bc_params, \n",
    "    X, y\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, we generally should not increase complexity of modeling with ensemble methods if DecisionTree already provided great results. Here we see although, the amount of false positives has been greatly reduced, it was at the expense of accuracy and a huge increase in false negatives. Even if the goal is to optimize for specificity, the inbalance of false values is a great concern for the merits of this model. Since Bagging Trees' result kind of regressed, it would not be beneficial to proceed with the other variations that involve bootstrapping and we predict those models, RandomForest, and Extra Trees, will not yeild any interesting results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_params = {'booster': ['gbtree'],\n",
    "             'max_depth': [3],\n",
    "             'learning_rate': [.01]}\n",
    "\n",
    "model_xg = gridSearchHelper(\n",
    "    xgb.XGBClassifier(random_state = random_state), \n",
    "    boost_params, \n",
    "    X, y\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final model we wanted to attempt was the XGBoost (Extreme Gradient Boost) model. While the model performed great, it did not provide any improvement over the DecisionTree model used above. Furthermore, the cores appeared to mirror the DecisiontTree. Although XGBoost was proclaimed to be an extremely robust model that has been proven to be very accurate when used, in this scenario it unforunately did not deliver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATH 2: Feature Reduction\n",
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use `interact`  \n",
    "to make predictive zip code affluency function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Query\n",
    "\n",
    "With the model of choice in hand, we want to create a query for our client to access to quickly determine if a particular area is affluent so that resources can be distributed properly in times of disaster. Below, a user can input either a zip code of choice or city and it will return that area with its affluency status attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_df[incomplete_df['zip_code'] == 11249]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gridSearchHelper(\n",
    "    BaggingClassifier(random_state = random_state), \n",
    "    bc_params, \n",
    "    X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the best model above in preparation for creating the query function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(user_input):\n",
    "    if type(user_input) == int: # the first check in the function would be to see if the input is a string or an integer\n",
    "                                # this is to enable the query take both type of values and return their appropriate result\n",
    "            \n",
    "        try: # We want to filter out any non-NYC cities or zipcodes and return a message that this query will only work for NYC area\n",
    "            \n",
    "            if incomplete_df[incomplete_df['zip_code'] == user_input]['affluency_rate'].isnull().sum() == 0:\n",
    "            # For integeer type data, the input will be a zipcode; we want to check if that particular zip code\n",
    "            # already has the affluency rate generated from the IRS data above. If the data exist, we can directly\n",
    "            # pull the data and display it from the dataframe\n",
    "                \n",
    "                return pd.DataFrame({'zip_code': user_input, \n",
    "                                     'affluency_rate': incomplete_df[incomplete_df['zip_code'] == user_input]['affluency_rate'].iloc[0]}, \n",
    "                                    index = range(1))\n",
    "                # If the above condition is met, the function will return a dataframe for that particular zipcode        \n",
    "                \n",
    "            else: # If the zipcode given did not have affluency rate given, we can use our model above to predict\n",
    "                  # the affluency rate\n",
    "                    \n",
    "                X_incomplete_dummy = incomplete_df[incomplete_df['zip_code'] == user_input]\n",
    "                # We are instantiating a dummy local datagrame for all data within that particular zipcode\n",
    "                \n",
    "                X_zip_column = X_incomplete_dummy[X.columns]\n",
    "                # Because of the nature of the model (XGBoost), it is required that the features MUST be in the same order\n",
    "                # for both fitted data and predicting data. We can solve that by setting the features in the same order as our\n",
    "                # model by retrieving the features from model preparation section above\n",
    "                \n",
    "                y_pred = model.predict(X_zip_column)\n",
    "                # We retrieve our predict values from the incomplete file and instantiate that to a variable\n",
    "                \n",
    "                return pd.DataFrame({'zip_code': user_input, 'affluency_rate': y_pred.mean()}, \n",
    "                                    index= range(1))\n",
    "                # We return a single row dataframe that outputs the zipcode with its associated affluency rate by taking the \n",
    "                # mean of all predicted value (0s and 1s) within the zipcode\n",
    "        except:\n",
    "            print('Please enter a valid NYC zipcode.') #returning a prompt for invalid NYC zipcode input\n",
    "        \n",
    "    else: # If the name of the city (neighborhood or borough) is given, we can apply a different method to show the\n",
    "              # affluency rate for all zipcodes within that region\n",
    "        \n",
    "        if incomplete_df[incomplete_df['city'] == user_input].shape[0] != 0:\n",
    "            # We want to filter out cities that are not in NYC with this conditional statement\n",
    "            \n",
    "            city_df = incomplete_df[incomplete_df['city'] == user_input]\n",
    "            # We want to create a sub dataframe here to include only all data within that city\n",
    "\n",
    "            if city_df['affluency_rate'].isnull().sum() == 0: \n",
    "            # Similar to the zipcode application above, if all values of affluency within that data exists, then we will not \n",
    "            # need to use our model and predict affluency for that particular region\n",
    "\n",
    "                zip_code_afflcuency = []\n",
    "                # Here we are instantiating an empty list to store each observation for zip and affluency within that particular region\n",
    "\n",
    "                for i in city_df['zip_code'].unique():\n",
    "                # Because each zip code is repeated multiple times per business, we only need the unique zipcodes within that data\n",
    "\n",
    "                    zip_code_afflcuency.append({'zip_code': i, \n",
    "                                                'affluency_rate': city_df[city_df['zip_code'] == i]['affluency_rate'].iloc[0]})\n",
    "                    # For each zipcode that it gets looped through, we only need to take the first value for that zipcode range\n",
    "                    # and append it to the list. The reason for this is the affluency rate for a single zipcode is always the same.\n",
    "                    # We only need to show each zipcode's affluency rate for a given region queried.\n",
    "\n",
    "                return pd.DataFrame(zip_code_afflcuency)\n",
    "                # This will return the list created above into a dataframe showing all the zipcodes and their associated affluency rate\n",
    "\n",
    "            else: # As per above, we have to consider certain regions where some zipcodes did not have their affluency rate provided\n",
    "                  # by the IRS data\n",
    "                city_affluency = []\n",
    "                # Here we also instantiate an empty list to store the zipcodes and their affluency rate after extraction\n",
    "\n",
    "                for i in city_df['zip_code'].unique():\n",
    "                # As above, we want to only look at unique zip codes since there will be repeats throughout the dataframe\n",
    "\n",
    "                    if city_df[city_df['zip_code'] == i]['affluency_rate'].isnull().sum() == 0:\n",
    "                    # for the zipcodes that already has affluency rate, we can isolate those values with the condition set above\n",
    "                        city_affluency.append({'zip_code': i, \n",
    "                                               'affluency_rate': city_df[city_df['zip_code'] == i]['affluency_rate'].iloc[0]})\n",
    "                        # we will store all those values within the empty list instantiated above \n",
    "\n",
    "                    else: # for the remaining values (i.e. data that were not given by the IRS) we will use our model to predict\n",
    "                          # the affluency rate for all zipcodes within that region\n",
    "                        X_complete = complete_df.drop(columns=remove_columns+[target])\n",
    "                        # As per above, we need to set the columns in the same order for fitting and predicting\n",
    "                        \n",
    "                        X_incomplete_dummy = city_df[incomplete_df['zip_code'] == i]\n",
    "                        # Instantiating the dataframe for the particular zipcode as a variable\n",
    "                        \n",
    "                        X_zip_column = X_incomplete_dummy[X.columns]\n",
    "                        # Matching the feature order as the model fitted\n",
    "                        \n",
    "                        y_pred = model.predict(X_zip_column)\n",
    "                        # creating an array of predicted values\n",
    "                        \n",
    "                        city_affluency.append({'zip_code': i, 'affluency_rate': y_pred.mean()})\n",
    "                        # Appending the list above with the predicted values as an average of affluency\n",
    "\n",
    "                return pd.DataFrame(city_affluency)\n",
    "                # returning the dataframe for that particular region\n",
    "        \n",
    "        else: # For cities entered that are not within NYC, the prompt below will appear to alert the user of the error\n",
    "            print(\"Please enter a proper city name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query operates with an input of either a zip or a string of the city's name. This will return a dataframe with the necessary information of that area's affluency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(10010) #Query showing affluency rate for Flatiron here at Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query('Bronx') # Query showing affluency rate for every zipcode in Bronx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query('Las Vegas') # An invalid city name will prompt a message to input a proper name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(89147) # An invalid zipcode will prompt a message to please input a proper NYC zipcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We want to evaluate our model that performed the best in terms of specificity and accuracy. Using the query function created from above, we can examine the output for zip codes that did not have an affluency rate given by the IRS data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(11249)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/11249.png\" alt=\"drawing\" width=\"720\"/>\n",
    "\n",
    "In the query above, the model determined that particular zip code is close to the .15 threshold. Our model predicted this area to have an affluency rate of 11%. We can verify the model [here](https://newyork.hometownlocator.com/zip-codes/data,zipcode,11249.cfm) and see that the average income per household is \\\\$90,853 with the median household income at \\\\$57,389. While we are not provided with the proportion of household incomes per income bracket, the area observed is within middle class range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(11239)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/11239.png\" alt=\"drawing\" width=\"720\"/>\n",
    "\n",
    "The query for the zip code above predicted that partcular zip is not affluency for any of the businesses as a data point. A verification can be show via [this link](https://www.zipdatamaps.com/11239) where the median housefhold income is \\\\$21,116 and the average household income is \\\\$39,110. With income ranges that low, our model appears to accurately classified this neigborhood to be low income based and would benefit from additional disaster relief aid in times of crisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query(10459)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/10459.png\" alt=\"drawing\" width=\"720\"/>\n",
    "\n",
    "Our final evaluation of the model shows the zip code above to be not affluent. According this [data](https://www.zipdatamaps.com/10455), the average household income is \\\\$27,060 and the median income is \\\\$24,406. Similar to the previous neighborhood, this area was classified by our model to be well below our affluence threshold. Providing aid to this neighborhood as a priority would be crucial when disaster occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query(10020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/10020.png\" alt=\"drawing\" width=\"720\"/>\n",
    "\n",
    "The last query we want to present here is a commercial zip code (i.e. all the buildings within this zip code do not have residential capabilities). 10020 was predicted to be a highly affluent area, which is correct (This zip code inhibits the Rockefeller Center Building) and would be categorized as high valued. Likewise, businesses around this area would be considered to be high priced consumer services which coresponds to great amount of affluency. In times of disaster, we can confidently say that this area would probably not need immediate aid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations\n",
    "- [All]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Documentation\n",
    "- [NYC zip codes](https://www.nycbynatives.com/nyc_info/new_york_city_zip_codes.php)\n",
    "- [Yelp API - Business Endpoints](https://www.yelp.com/fusion)\n",
    "- [IRS dataset](https://www.irs.gov/statistics/soi-tax-stats-individual-income-tax-statistics-2016-zip-code-data-soi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO \n",
    "- Clean up imports\n",
    "- Organize the Table of Contents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
