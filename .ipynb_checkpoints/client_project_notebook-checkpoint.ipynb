{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Areas of Affluence using Yelp Pricing Data\n",
    "\n",
    "#### Authors: \n",
    "- Eddie Yip [LinkedIn](https://www.linkedin.com/in/eddie-yip-2a37324b/) | [Medium](https://medium.com/@eddie.yip2)\n",
    "- Hadi Morrow [LinkedIn](https://www.linkedin.com/in/hadi-morrow-4b94164b/) | [GitHub](https://github.com/HadiMorrow) | [Medium](https://medium.com/@hadi.a.morrow)\n",
    "- Mahdi Shadkam-Farrokhi: [GitHub](https://github.com/Shaddyjr) | [Medium](https://medium.com/@mahdis.pw) | [http://mahdis.pw](http://mahdis.pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement [Hadi]\n",
    "\n",
    "While affluence should never be a factor when choosing to provide disaster aid or not, we must consider the following:\n",
    "\n",
    "- On the assumption that affluence plays a role, one might relate affluency to preparedness. Those who can afford to will always look out for their families at any cost. Those who can not might not be able to prepare as well due to the fact that it is not an option. \n",
    "\n",
    "- On the assumption that affluence is not part of a majority class, if we should be miopic with our search efforts we might want to consider saving the masses, those living in tight coridors and those with little to no income. If effect those most suseptible to losing their lives in a major disaster. \n",
    "\n",
    "- Using tax data we aim to show that using YELP data dollar signs is enough to predict where we might want to quickly and accuratly align our efforts. \n",
    "\n",
    "New Light Technologies as our audience, we hope to show that while using expensive and hard to handle data such as tax data can be more precise, a quick and dirty aproach could be to simply sord though the dollar signs data on yelp. \n",
    "\n",
    "---\n",
    "[Hadi] - Excellent write up! Here are some of my observations - feel free to include or not, totally up to you.\n",
    "1. It would be nice for the reader if we define 'affluence' here at the start. What do we consider \"affluent\" in our data (I think we mentioned 15% of the area code?)?\n",
    "2. As a reader, it would be VERY compelling to have an actual case where a natural disaster occured and the affluent areas weren't affected. If possible, research 1 or 2 cases when affluent areas were better prepared for natural disasters - this will help prove our predictive model has a real use case.\n",
    "3. Tying into the use case, it might be helpful to mention a realistic disaster scenario when only having Yelp! price data would be useful. Like, there's an emergency and there's little time to pull granular information about the area, but knowing the yelp reviews for an area, allows first-responders to know which areas their efforts will have the most impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary [Mahdi]\n",
    "\n",
    "- Difficulty gathering data\n",
    "- Prompt confusing regarding \"affluence\"\n",
    "- Other projects used outside data as metric\n",
    "- We pulled from API and didn't use old data, which was challenging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Gathering Data](#Gathering-Data)\n",
    "- [Loading Data](#Loading-Data)\n",
    "- [Preliminary Exploratory Data Analysis](#Preliminary-Exploratory-Data-Analysis)\n",
    "- [Cleaning the Data](#Cleaning-the-Data)\n",
    "- [Feature Engineering](#Feature-Engineering)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "- [Model Preparation](#Model-Preparation)\n",
    "- [Model Selection](#Model-Selection)\n",
    "- [Model Evaluation](#Model-Evaluation)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)\n",
    "- [Source Documentation](#Source-Documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "We got Yelp data using the API - link \n",
    " \n",
    "We got IRS data using - source [Eddie]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "- [All]\n",
    "- For map visual, will need [Basemap](https://rabernat.github.io/research_computing/intro-to-basemap.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import columnExpander\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = \"./data/total_merged.csv\"\n",
    "df_yelp = pd.read_csv(data_file_path, index_col = 0)\n",
    "df_yelp.reset_index(drop=True, inplace = True) # same indeces were merged using multiple API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Exploratory Data Analysis\n",
    "- [All]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_null = df_yelp.isnull().sum()\n",
    "sum_null[sum_null > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have many missing values in the data, however many of the columns are not meaningful for our problem and these columns can be safely dropped.\n",
    "\n",
    "Also, `categories`, `location`, and `transactions` are compressed data columns and will need to be unpacked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "- [Mahdi] one person for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[\"price\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to drop null prices from analysis as this is the key indicator we're looking to predict with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.dropna(subset=[\"price\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Yelp Price to ordinal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['price'] = df_yelp['price'].map({'$': 1, '$$': 2, '$$$': 3, '$$$$':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['price'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unneccessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keepers = ['categories','id', 'location', 'price', 'rating', 'review_count', 'transactions', 'coordinates']\n",
    "df_yelp = df_yelp[keepers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_sting_dict(string, keys):\n",
    "    if len(string) == 0:\n",
    "        return None\n",
    "    dic = literal_eval(string)\n",
    "    out = {}\n",
    "    for key in keys:\n",
    "        out[key] = dic.get(key)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"location\"\n",
    "keys = [\"zip_code\", \"city\", 'state']\n",
    "zips_and_cities = df_yelp[location].map(lambda string: get_keys_from_sting_dict(string, keys))\n",
    "\n",
    "for key in keys:\n",
    "    df_yelp[key] = [pair[key] for pair in zips_and_cities]\n",
    "    \n",
    "df_yelp.drop(columns=[location], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for NYC-only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed non-NY state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = df_yelp[df_yelp['state'] == \"NY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing zip codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[df_yelp['zip_code'] == \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locations were found using Google Maps and zip codes imputed manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.loc[df_yelp[\"id\"] == \"ECY0sIYxPJio81dteqiMhg\",\"zip_code\"] = \"10007\"\n",
    "df_yelp.loc[df_yelp[\"id\"] == \"6u5cnsN35mJz24HMQ9pfFw\",\"zip_code\"] = \"10314\"\n",
    "df_yelp.loc[df_yelp[\"id\"] == \"BilbRcNQXKmcBFvLm4gxAQ\",\"zip_code\"] = \"11372\"\n",
    "df_yelp.loc[df_yelp[\"id\"] == \"jZzbV6SRt9FXdCoziNv5xw\",\"zip_code\"] = \"11373\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove by NYC zip\n",
    "We used the range of zip codes designated for NYC - [source](https://www.nycbynatives.com/nyc_info/new_york_city_zip_codes.php)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_zip = 10001\n",
    "max_zip = 11697\n",
    "\n",
    "df_yelp['zip_code'] = df_yelp['zip_code'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = df_yelp[(df_yelp['zip_code'] >= min_zip) & (df_yelp['zip_code'] <= max_zip)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = \"coordinates\"\n",
    "coord_keys = [\"latitude\", \"longitude\"]\n",
    "lat_and_long = df_yelp[coordinates].map(lambda string: get_keys_from_sting_dict(string, coord_keys))\n",
    "\n",
    "for key in coord_keys:\n",
    "    df_yelp[key] = [pair[key] for pair in lat_and_long]\n",
    "\n",
    "df_yelp.drop(columns=[coordinates], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[coord_keys].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the latitude and longitude, we see some points that do not appear to be in the New York City area, around 40.7 and -73.9, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[(df_yelp[\"latitude\"] > 41) | (df_yelp[\"latitude\"] < 40)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data point is from Scarsdale NY, which is not within the city limits. This data point will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = df_yelp[(df_yelp[\"latitude\"] <= 41) & (df_yelp[\"latitude\"] >= 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[(df_yelp[\"longitude\"] > -72) | (df_yelp[\"longitude\"] < -75)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking up these businesses, it is clear they were given a positive longitude when they are actually supposed to be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.loc[(df_yelp[\"longitude\"] > -72) | (df_yelp[\"longitude\"] < -75),\"longitude\"] = np.negative(df_yelp[(df_yelp[\"longitude\"] > -72) | (df_yelp[\"longitude\"] < -75)][\"longitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_dict_to_string(string, key):\n",
    "    return \",\".join([dic[key] for dic in literal_eval(string)])\n",
    "\n",
    "df_yelp[\"categories\"] = df_yelp[\"categories\"].map(lambda s: convert_string_dict_to_string(s,\"alias\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_list_to_string(string):\n",
    "    return \",\".join(literal_eval(string))\n",
    "\n",
    "df_yelp[\"transactions\"] = df_yelp[\"transactions\"].map(convert_string_list_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values - this is a complete dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning IRS Dataset [Hadi]\n",
    "These data were collected directly from the IRS website ([source](https://www.irs.gov/statistics/soi-tax-stats-individual-income-tax-statistics-2016-zip-code-data-soi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irs = pd.read_csv('./data/irs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irs[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_zips = list(set(df_yelp['zip_code']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str_num(str_num):\n",
    "    '''Returns integer of input string with commas removed'''\n",
    "    return int(str_num.replace(',',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affluency_rates = []\n",
    "found_zips = []\n",
    "missing_zips = []\n",
    "returns_col_name = 'Number of returns'\n",
    "\n",
    "for zip_code in yelp_zips:\n",
    "    try:\n",
    "        sub_df               = df_irs[df_irs.iloc[:,0] == str(zip_code)]\n",
    "        \n",
    "        affluent_irs_returns = clean_str_num(sub_df[returns_col_name].iloc[-1])\n",
    "        total_irs_returns    = clean_str_num(sub_df[returns_col_name].iloc[0])\n",
    "        affluent_rate        = affluent_irs_returns / total_irs_returns\n",
    "        \n",
    "        affluency_rates.append(affluent_rate)\n",
    "        found_zips.append(zip_code)\n",
    "    except Exception as e:\n",
    "        missing_zips.append(zip_code)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_zips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 26 zip codes in the yelp data that were not found in the IRS dataset.  \n",
    "These associated datapoint will be dropped, as they have not target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affluency_df = pd.DataFrame(data = {\"zip_code\": found_zips, \"affluency_rate\":affluency_rates})\n",
    "affluency_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Yelp and IRS dataset\n",
    "Merging the yelp dataset with the IRS dataset will drop those observations with missing zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_yelp, affluency_df, on = \"zip_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_int = [\"review_count\",\"rating\"]\n",
    "df[convert_to_int] = df[convert_to_int].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left with 8869 complete data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "- [All]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansion_columns = [\"categories\",\"transactions\"]\n",
    "lce = columnExpander.ListColumnExpander(expansion_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.DataFrame(lce.fit_transform(df).toarray(), columns=lce.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.concat([df, dummy_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.concat([df.drop(columns = expansion_columns), dummy_df], axis=1)\n",
    "complete_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Latitude and Longitude using clustering\n",
    "We decided to reduce the latitude and longitude variables into a single column using $K$-means clustering as a way of representing more local neighborhoods.\n",
    "\n",
    "An optimum $k$ will be determined using the model's silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = 10 ## will figure out later\n",
    "km = KMeans(n_clusters=best_k)\n",
    "lat_long_df = complete_df[[\"latitude\",\"longitude\"]]\n",
    "km.fit(lat_long_df)\n",
    "complete_df[\"location_cluster\"] = km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Affluency Threshold\n",
    "Affluency = when a zip code has 15% of its population file and IRS return of $$200k or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affluency_thresh = .15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df[\"is_affluent\"] = (complete_df[\"affluency_rate\"] >= affluency_thresh).astype(int)\n",
    "complete_df[\"is_affluent\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 28% of all reported IRS returns in New York City count as being affluent, according to our definition.\n",
    "\n",
    "This leads our data to be somewhat unbalanced, which we need to keep in mind when evaluating our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "- [Mahdi] and [Hadi] killer graphs and visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is work in progress\n",
    "# ============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scoop lat/long up in a matrix so we can use them easily\n",
    "location_data = df[[\"latitude\", \"longitude\"]].astype(float)\n",
    "location_data.head()\n",
    "location_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's cluster our observations by lat/long\n",
    "km = KMeans(n_clusters=20)\n",
    "km.fit(location_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are the highly prices houses?\n",
    "location_data.plot(kind=\"scatter\", x=\"latitude\", y=\"longitude\",\n",
    "         cmap=\"inferno\", figsize=(14, 10), s=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do these clusters look like visually?\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter((location_data.latitude).astype(float), (location_data.longitude).astype(float), c=km.labels_, s=50 ,cmap=\"tab20\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "# Extract the data we're interested in\n",
    "df = df.dropna()\n",
    "lat = df['latitude'].values\n",
    "lon = df['longitude'].values\n",
    "price = df['price'].values\n",
    "rating = df['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Draw the map background\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "m = Basemap(resolution = 'i', projection = 'cyl',\n",
    "            llcrnrlat=40.35868, llcrnrlon=-74.491350,\n",
    "            urcrnrlat = 41.163819, urcrnrlon = -73.447253,epsg = 3395)\n",
    "m.shadedrelief()\n",
    "m.arcgisimage(service =\"ESRI_Imagery_World_2D\", xpixels = 2000)\n",
    "# 2. scatter city data, with color reflecting population\n",
    "# and size reflecting area\n",
    "m.scatter(lon, lat, latlon=True,\n",
    "          c=(price)**3, s=(rating)*4,\n",
    "          cmap='Reds', alpha=0.5)\n",
    "\n",
    "# 3. arcgisimage\n",
    "plt.savefig(\"ESRI_Imagery_World_2D\", bbox_inches = \"tight\", dpi = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test-split again\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.copy(), y.copy(), random_state=42, test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How'd we do now?\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All this wrapped up!\n",
    "def transfer_tune(X, y, k):\n",
    "    location_data = df_yelp[[\"latitude\", \"longitude\"]]\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(location_data)\n",
    "    X.loc[:, \"cluster\"] = km.predict(location_data)\n",
    "    X_dummy = pd.get_dummies(columns=[\"cluster\"], data=X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_dummy.copy(), y.copy(), random_state=42, test_size=0.5\n",
    "    )\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    r2 = model.score(X_test, y_test)\n",
    "    print(f\"{k} : {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2, 103, 5):\n",
    "    transfer_tune(X, y, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = df, x = 'affluency_rate', y = 'zip_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============\n",
    "# Above is work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/Loading Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_df.to_csv(\"./data/clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.read_csv(\"./data/clean_data.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns = [\n",
    "    'id',\n",
    "    'zip_code',\n",
    "    'city',\n",
    "    'state',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'affluency_rate',\n",
    "    'transactions_'\n",
    "]\n",
    "target = 'is_affluent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = complete_df.drop(columns=remove_columns+[target])\n",
    "y = complete_df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "- [Hadi] Exploring models\n",
    "- [Eddie] Exploring models\n",
    "\n",
    "Maybe split on which models you 2 want to try out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "- [Mahdi] killer graphs and visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations\n",
    "- [All]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Documentation\n",
    "- [NYC zip codes](https://www.nycbynatives.com/nyc_info/new_york_city_zip_codes.php)\n",
    "- [Yelp API - Business Endpoints](https://www.yelp.com/fusion)\n",
    "- [IRS dataset](https://www.irs.gov/statistics/soi-tax-stats-individual-income-tax-statistics-2016-zip-code-data-soi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
